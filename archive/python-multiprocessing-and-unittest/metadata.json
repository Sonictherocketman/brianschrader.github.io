{"status":"publish","pubdate":"Tue, 28 Apr 2015 01:39:03 -0000","tags":"python, bugs","author":"Brian Schrader","content":"<p>I've been having an issue with unit testing Microblogger when my tests need to use Python's multiprocessing module. I've been looking at this code for days now and I can't seem to find the bug. I'm hoping that by writing down my thoughts here, I can think through the problem. </p>\n<p>Basically, the test is trying to verify that a User object can be created with information from a remote XML feed. The <a href=\"https://github.com/Sonictherocketman/Microblogger/blob/master/test/user_test.py#L41\">test</a> gives the <code>User</code> module a URL and tells it to fetch all information at that resource.</p>\n<p><code class=\"python\"><pre>    def test_cache_user(self):\n        user = User(remote_url='http://microblog.brianschrader.com/feed')\n        user.cache_user()\n        self.assertEqual(user._status, dl.CACHED)\n        self.assertEqual(user.username, 'sonicrocketman')\n </pre></code></p>\n<p>The <a href=\"https://github.com/Sonictherocketman/Microblogger/blob/master/feed/user.py#L84\"><code>cache_user</code></a> method starts up a crawler to go out and parse the contents of the URL provided.</p>\n<p><code class=\"python\"><pre>    def cache_users(users):\n        ...\n        from crawler.crawler import OnDemandCrawler\n        remote_links = [user._feed_url for user in users]\n        user_dicts = OnDemandCrawler().get_user_info(remote_links)\n        ...\n</pre></code></p>\n<p>Everything is ok still. Inside that <code>OnDemandCrawler().get_user_info()</code> method, the OnDemandCrawler crawls the URL given and then calls <a href=\"https://github.com/Sonictherocketman/Microblogger/blob/master/crawler/crawler.py#L71\"><code>self.on_finish()</code></a>. This is when things get funky.</p>\n<p><code class=\"python\"><pre>    def on_finish(self):\n        self.stop(now=True)\n</pre></code></p>\n<p>The stop command tells the crawler to shut down, the <code>now</code> keyword just tells it to force stop the crawling process and don't wait to cleanly exit.</p>\n<p>If we look at the source to the <a href=\"https://pypi.python.org/pypi/MicroblogCrawler/1.4.1\">microblogcrawler (v1.4.1)</a> we see that <a href=\"https://github.com/Sonictherocketman/microblog_crawler/blob/master/microblogcrawler/crawler.py#L116\"><code>stop</code></a> does the following:</p>\n<p><code class=\"python\"><pre>    def stop(self, now=False):\n        ...\n        if now:\n            # Try to close the crawler and if it fails,\n            # then ignore the error. This is a known issue\n            # with Python multiprocessing.\n            try:\n                self._stop_crawling = True\n                self._pool.close()\n                self._pool.join()\n            except:\n                pass\n        ...\n</pre></code></p>\n<p>The curious part is that <code>self._stop_crawling = True</code> part. In the <a href=\"https://github.com/Sonictherocketman/microblog_crawler/tree/master/test\">tests for the microblogcrawler</a> both forcing the crawler to stop <em>and</em> normally stopping it work fine. The issue arises when trying to stop them in a unit test. For some reason the crawler doesn't stop. </p>\n<p>Here's a sample crawler and the output it produces when run as a unit test:</p>\n<p><code class=\"python\"><pre>    class SomeCrawler(FeedCrawler):\n        def on_start(self):\n            print 'Starting up...' + str(self._stop_crawling)\n        def on_finish(self):\n            print 'Finishing up...' + str(self._stop_crawling)\n            self.stop()\n            print 'Should be done now...' + str(self._stop_crawling)\n</pre></code></p>\n<p><code class=\"python\"><pre>>>> python -m crawler_test\n>>> Starting up...False        # Correct\n>>> Finishing up...False       # Correct\n>>> Should be done now...True  # Correct\n>>> Starting up...False        # lolwut?\n</pre></code></p>\n<p>For some reason the crawler isn't receiving the signal to stop. Looking at it from my Activity Monitor it appears to stop (the 4 worker threads are closed), but then the crawler creates 4 new worker threads and does it all over again. </p>\n<p>The last step of this process is inside the crawler itself. The <a href=\"https://github.com/Sonictherocketman/microblog_crawler/blob/master/microblogcrawler/crawler.py#L184\">crawling process</a> is controlled by the <code>self._stop_crawling</code> attribute:</p>\n<p><code class=\"python\"><pre>    def _do_crawl(self):\n        ...\n        # Start crawling.\n        while not self._stop_crawling:\n            # Do work...\n            ...\n            self.on_finish()\n</pre></code></p>\n<p>From this code, if the <code>_stop_crawling</code> attribute is set to <code>True</code>, then the crawler should finish the round it's on and close down, but the value of the attribute doesn't seem to be sticking when it's assigned in the <code>stop</code> method above.</p>\n<p>If anyone has any ideas as to what the issue could be, I'd love to hear them. I'm pretty much out of ideas now. As I said before, the tests in the microblog crawler (which are not unit tests) work fine. The issue only comes up when running a test suite through <code>unittest</code> itself.</p>\n<p><link rel=\"stylesheet\" href=\"http://yandex.st/highlightjs/8.0/styles/default.min.css\">\n<script src=\"http://yandex.st/highlightjs/8.0/highlight.min.js\"></script>\n<script>hljs.initHighlightingOnLoad();</script></p>","link":"http://brianschrader.com/archive/python-multiprocessing-and-unittest","location":"/archive/python-multiprocessing-and-unittest/","published":"Tue, 28 Apr 2015 ","title":"Python multiprocessing and unittest","slug":"python-multiprocessing-and-unittest","article-number":98}