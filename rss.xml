<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0"><channel><title>BiteofanApple</title><link>http://brianschrader.com</link><description>Thoughts, and ramblings about technology, programming, history,
 and whatever else I'm interested in.</description><language>en-US</language><image><url>https://www.gravatar.com/avatar/11b074a636e00292c98e3e60f7e16595</url><title>Brian Schrader</title><link>http://brianschrader.com</link></image><lastBuildDate>Sun, 10 Apr 2022 06:55:54 +0000</lastBuildDate><generator>sonicrocketman-feed-generator/v2.0</generator><atom:link href="https://brianschrader.com/rss.xml" rel="self" type="application/rss+xml"/><category>programming</category><category>history</category><category>science</category><category>personal</category><category>technology</category><item><guid isPermaLink="true">http://brianschrader.com/archive/hacks-can-be-good-code-too</guid><pubDate>Mon, 21 Mar 2022 21:45:00 -0800</pubDate><category>programming</category><category>software development</category><title>Hacks can be Good Code Too</title><link>http://brianschrader.com/archive/hacks-can-be-good-code-too/</link><description><![CDATA[<style>
table {
    width: 100%;
}
table td {
    padding: 2px;
    padding-right: 1.5rem;
}
table tr td:last-child {
    padding: 0;
}
</style>

<p>Writing code is, like everything in life, all about making tradeoffs. Code can be quick to write, but at the same time unreadable; it can be fast, but hard to maintain; and it can be flexible, but overly complex. Each of these factors are worth considering when writing Good Code. Complicating this is the fact that: what constitutes Good Code in one situation may not be ideal in another.</p>
<p>Good Code is not universally so.</p>
<p>It is incredibly difficult to explain why one set of tradeoffs are worth pursuing in one case but not in another, and often times reasonable people will disagree on the value of certain tradeoffs over others. Perhaps a snippet of hacky string parsing is good in one place, but not in another. Often times, the most significant cost of solving a problem "The Right Way" is time.</p>
<p>When deciding whether to do something The Right Way or to cheat and simply hack something together, I often try to consider the exposure the given code will have. Consider these questions:</p>
<ul>
<li>Do other systems touch this code?</li>
<li>How many developers will need to interact with it over time?</li>
<li>How much work would be involved in building out the correct approach?</li>
<li>How much work would be involved in building out the bad approach?</li>
<li>How valuable is the intended feature?</li>
<li>How much additional maintenance does the bad solution require?</li>
</ul>
<p>Each of these answers helps me decide what kind of code I should write. These questions neglect multiple other factors (e.g. performance, readability), but they are a good starting point.</p>
<p>In a recent example I needed to modify the blog engine that powers this site as well as a few others. I wanted a simple feature that would count the number of articles on the side as well as the total number of words in every blog post, and display those values on the home page. <a href="/archive/the-new-new-cms/">As I've said before</a> the blog engine for this site is very old, and has been <a href="/archive/smaller/">rewritten</a> <a href="/archive/rewritten/">several</a> <a href="/archive/thinking-about-redoing-my-blog-engine/">times</a>. It's well beyond needing a massive rewrite, but that's not something I really want to do right now.</p>
<p>The blog engine is written in Python, provides a command-line interface, and uses Git Hooks both client and server-side to build and deploy itself.</p>
<p>I originally considered writing this feature in Python: counting the number of words in each article, adding a new context variable to the template rendering process, and then rendering the pages as normal. But that would require touching substantial pieces of the codebase (some of which <a href="/archive/i-solved-the-same-bug-twice-and-didnt-know-it/">I no longer understand</a>). It would probably take me all evening to dive into the code, understand it, make the change, and test it. To be honest, this feature was not worth wasting an evening on. So I decided to just hack something.</p>
<p>As I said, I use Git to deploy the site. So I just added a new line to the HTML template:</p>
<pre><code class="html">
&lt;p&gt;
    This site contains <code>{+ARTICLE_COUNT+}</code>
    different writings and <code>{+WORD_COUNT+}</code>
    total words. That's about <code>{+PAGE_COUNT+}</code>
    pages!
&lt;/p&gt;
</code></pre>

<p>And then I added a new step to the pre-commit hook that runs after the template rendering process, but before the changes are committed and the site is deployed.</p>
<pre><code class="bash">
WPP=320
WORDS_N="$(find archive/ -name "*.md"|xargs -I {} cat {} | wc -w)"
WORDS=`printf "%'d" $WORDS_N`
ARTICLES=`printf "%'d" $(find archive/ -name "*.md" | wc -l)`
PAGES="$(( WORDS_N / WPP ))"

TMP_HOME=`mktemp`
cp ./index.html $TMP_HOME
cat $TMP_HOME |
    sed "s/{+ARTICLE_COUNT+}/$ARTICLES/" |
    sed "s/{+PAGE_COUNT+}/$PAGES/" |
    sed "s/{+WORD_COUNT+}/$WORDS/" > ./index.html
</code></pre>

<p>Let's check in and see how this hack fit my criteria above:</p>
<table border="1" frame="hsides" rules="rows" style="margin-right:auto;margin-left:auto;">
    <tr>
        <td>Do other systems touch this code?</td>
        <td>No</td>
    </tr>
    <tr>
        <td># of Developers?</td>
        <td>1</td>
    </tr>
    <tr>
        <td>Time for Correct Approach?</td>
        <td>2-3 hours</td>
    </tr>
    <tr>
        <td>Time for Bad Approach?</td>
        <td>10 minutes</td>
    </tr>
    <tr>
        <td>How Valuable is the Feature?</td>
        <td>Very</td>
    </tr>
    <tr>
        <td>Additional Maintenance Burden?</td>
        <td>Not much</td>
    </tr>
</table>

<p>Is this elegant: absolutely not. Did it take basically zero time? Yes. Have I thought about it since? Not until writing this post. Would I have done this on a team project or a commercial product? Absolutely not. It's a feature for my personal blog engine and a feature that is specific to one particular low-value site that I run.</p>
<p>In this case, a hack is an example of Good Code. That's because Good Code is a relative construct.</p>
<p><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script></p>
<script>hljs.initHighlightingOnLoad();</script>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/at-vs-on-a-story-of-semantic-data-modeling</guid><pubDate>Mon, 21 Feb 2022 22:50:00 -0800</pubDate><category>programming</category><category>software development</category><title>At vs. On: a Story of Semantic Data Modeling</title><link>http://brianschrader.com/archive/at-vs-on-a-story-of-semantic-data-modeling/</link><description><![CDATA[<p>As most good software developers eventually learn: time is hard. Time-based bugs are incredibly common and are sometimes difficult to solve. There are a <a href="https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time">ton of misconceptions about how time and dates work</a> in the real world and the simple solution is rarely correct for any significant length of time. Performing calculations based on times and dates can get messy, but so can simply storing them. There's a lot to be wary of when building out a data model with timestamps involved, and as always, a lack of consistent naming can cause a ton of problems.</p>
<p>Over the years I've come to use a specific terminology for dates and time in my data models. In general, I prefer not to use data types in variable names, and I prefer my code to read as passable English where possible. This means, I tend to avoid names like: <code>date_created</code> or <code>published_ts</code> which contain the data type in the name, and I avoid names like: <code>created</code> which give me absolutely no indication of the type or what it is used for.</p>
<p>Instead, I prefer to take cues from the English language. For timestamps or any data type that represents a precise moment in time, I use the suffix <code>at</code>. For dates or times that represent more abstract things like wall time or calendar dates, I use the suffix <code>on</code>.</p>
<p>As an example let's say I have the following data model:</p>
<pre><code>class BlogPost:

    # ... other fields ...

    created_at = TimestampField()
    updated_at = TimestampField()

    posted_on = DateField()
</code></pre>
<p>This convention tells me that I should expect the <code>posted_on</code> field to contain a date or time but not both, and that it represents an abstract notion of time, whereas both the <code>created_at</code> and <code>updated_at</code> fields represent a specific moment.</p>
<p>I arrived at this convention through asking myself questions about the data in plain English. Consider the following questions:</p>
<ol>
<li>Q: When was this post published?<br />
A: It was published <strong>on</strong> the 25th of January.</li>
<li>Q: When was the post record created?<br />
A: It was created <strong>at</strong> 12:00 PM on January 24th.</li>
</ol>
<details><summary>Disclaimer</summary>
This convention doesn't always work because usually people would use <b>at</b> to describe any time (e.g. "I arrived <b>at</b> noon"). But once I settled on the convention, it wasn't confusing. It just doesn't always read nicely.
</details>

<p>Knowing when to use a timestamp vs. a calendar date or wall-clock time is another issue (and a complicated one), but at least with this convention, I know which one I'm dealing with.</p>
<div class="footnote">
Now that I think about it, it might make sense to name timestamps with an <b>aton</b> suffix since question #2 technically uses both <b>at</b> and <b>on</b>.
</div>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/generating-deterministic-procedural-artwork-with-pdraw</guid><pubDate>Fri, 18 Feb 2022 22:36:00 -0800</pubDate><category>programming</category><category>python</category><category>software</category><category>tools</category><title>Generating Deterministic, Procedural Artwork with pdraw</title><link>http://brianschrader.com/archive/generating-deterministic-procedural-artwork-with-pdraw/</link><description><![CDATA[<p>I've been messing with procedural artwork lately, and I've decided to discuss the fruits of my labor. Behold <a href="https://github.com/Sonictherocketman/pdraw">pdraw</a>: a script that generates cool line art from arbitrary text!</p>
<p><a href="https://github.com/Sonictherocketman/pdraw"><img class="image-right" src="/images/blog/pdraw.gif" alt="Pdraw.py" style="max-width: 300px;" /></a></p>
<p>About two weeks ago, <a href="https://www.youtube.com/watch?v=tkC1HHuuk7c">Numberphile</a> released a new video about visualizing the digits of Pi using procedural artwork. I was taken by the idea and decided that I would, for fun, simply replicate their technique in Python and use it to plot random sections of Pi. It was a sort of goof-off project to occupy an evening. At the end of that evening, I had finished my script (through several iterations) and plotted various sections of Pi. I could now plot any stream of numbers. Then it hit me: all digital data can be represented as a base 10 string of numbers. I could draw anything.</p>
<p>The next evening, I set about making it so that my script could accept various command-line configurations and convert any text into its base 10 equivalent (with a small tweak for artistic reasons). Once I had that, it was time to start plotting anything I could think of. I've tried drawing binaries, zip files, websites, and a lot more.</p>
<p>I've <a href="https://github.com/Sonictherocketman/pdraw">open sourced</a> the code for pdraw &mdash; which uses no dependences because Python is cool like that &mdash; so you the technically inclined reader can draw your very own text streams and see if they produce anything interesting.</p>
<p>I've found that the cooler drawings tend to arise from text containing between 250-2000 characters, although larger files can be cool too. What's especially interesting is that you can almost see the structure of the data in the drawing as you watch it draw. For example, my blog archive page is basically a blob of header information, then a long and repetitive HTML table, and then another blob. This structure appears in the drawing when you plot the HTML of the page; the table rows appearing as little loops followed by lines in random directions.</p>
<p><code>$ curl https://brianschrader.com/archive/ | ./pdraw -e</code></p>
<p>Check it out, and let me know what you do with it. I'd love to know if anyone finds more cool things to do with pdraw, or if you generate some particularly cool drawings with it.</p>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/i-solved-the-same-bug-twice-and-didnt-know-it</guid><pubDate>Tue, 01 Feb 2022 23:44:00 -0800</pubDate><category>software development</category><category>programming</category><title>I Solved The Same Bug Twice And Didn't Know It</title><link>http://brianschrader.com/archive/i-solved-the-same-bug-twice-and-didnt-know-it/</link><description><![CDATA[<p>Human memory is incredibly lossy; the brain an imperfect storage medium.</p>
<p>I've <a href="/archive/comments-with-cited-references/">written before</a> about how I include links in my code comments to resources that helped me find unintuitive or convoluted solutions. These links are essentially the footnotes of my systems; both documentation and a debugging paper-trail for poor, future souls to follow.</p>
<p>However, I am an imperfect soul, and so not all of my hackish solutions are cited. This fact bit me the other day when I discovered a strange bug with a new feature in <a href="https://adventurerscodex.com/">Adventurer's Codex</a>.</p>
<h3>The Technical Details</h3>
<p>The actual issue was fairly nuanced and understanding it depends heavily on the details of our infrastructure, but the simple version goes like this:</p>
<p>We have two endpoints, one that returns a resource and one that returns the schema for our entire API. For whatever reason, in this particular case, requesting the resource changed the output of the schema the first time the resource was requested. This is obviously unexpected. The structure of an API shouldn't change when you call it right? Well, that's the thing. Technically the API wasn't actually changing, but the schema was ever so slightly different: the format of the schema for that resource had changed.</p>
<p><img
    alt="A funny joke"
    src="/images/blog/changed-outcome.jpg"
    class="image-right"
    style="max-width:320px;"
/></p>
<p>The schema is a hierarchical description of all of our API endpoints, with paths to describe how each resource is related. In this case, the schema would output two different paths to the same resource, but the data at both paths were identical.</p>
<p>For technical reasons, these paths matter to us. <strong>They must remain the same.</strong></p>
<h3>Following the Breadcrumb Trail</h3>
<p>After some DuckDuckGo-fu failed to turn up any useful results, I turned to the rest of the codebase. The problematic endpoint was very simple. Surely we'd implemented other similar functionality which didn't cause this behavior. Sure enough I found two such cases. Both of which contained the same strange, seemingly useless line of code.</p>
<p>Once I had added that seemingly useless code, the schema no longer changed. I had fixed the bug! I had discovered <em>how</em> to fix the bug, but not <em>why</em>. I could have stopped there. Someone else might have, but I needed to go further. I needed to find out how this unrelated line solved my problem.</p>
<p>For now, I had found a clue.</p>
<p>Being the primary author of this particular codebase I knew that such a strange implementation would probably have at least a code comment or a comment in the implementing commit that explained the weirdness. There was no code comment unfortunately, and neither was there one in the commit. However, by diving through the history of that particular file I found that there had been a code comment, right where I would have expected it to be, when the code was written; but it had been removed in another commit a few months back&mdash;<em>by me</em>.</p>
<p><img
    alt="The Fatefult Commit"
    src="/images/blog/ac-fateful-comment.png"
    class="image-center"
/></p>
<p>Unfortunately, the comment didn't link to an answer on the internet and subsequent searches have turned up nothing of use.</p>
<p>For now, I had found another clue&mdash;and a big one at that. I had also discovered something scarier: I had already found, fought, and beaten this bug before, and it was I that deleted the vital clue.</p>
<p>I had discovered that I was not just the detective but the victim and the murderer, and that the case had been closed three years prior.</p>
<h3>Answers Lost to the Mists of Time</h3>
<p>As strange as it may seem, I find that this kind of thing happens more often than we may like to admit. I am in a situation where I maintain most of the code I write, and so I get to live with my mistakes for years&mdash;going on a decade now. When I wrote the offending code and the comment that explained it, I was deep in the bowels of <a href="https://www.django-rest-framework.org">DRF</a> building out the Adventurer's Codex backend from whole cloth. Since then I've moved on to building other things. That code has now sat for years untouched, working as designed; and <a href="https://www.goodreads.com/quotes/376870-and-some-things-that-should-not-have-been-forgotten-were">some things which should not have been forgotten were lost</a>.</p>
<p>At time of writing this mystery is still unsolved; the clues leading to an end I cannot see. However I now know that at one point in the past I did know the cause of this issue and how to solve it. The solution lives on, but <a href="/archive/where-it-comes-from-nobody-knows/">the cause is lost</a>: a coder's <a href="https://en.wikipedia.org/wiki/Greek_fire">greek fire</a>.</p>
<p>The answer may be lost to time, as even three years is enough time for some links to rot and trails to run cold. In what now feels like another lifetime, a past version of myself held the answers I now seek. Perhaps one day a Future Me will know what Past Me had found.</p>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/take-a-break-script-something</guid><pubDate>Mon, 03 Jan 2022 04:58:00 -0800</pubDate><category>software development</category><category>programming</category><title>Take a Break, Script Something</title><link>http://brianschrader.com/archive/take-a-break-script-something/</link><description><![CDATA[<p>Lately I've been putting in a fair amount of work improving Adventurer's Codex, and we have some very exciting updates coming soon (no spoilers 🤫). A lot of the changes required modifying our existing single-page app or API backend, but one involved the creation of a totally new repository and a new set of public-facing pages. It's those pages that I want to talk about.</p>
<p>The problem was fairly simple: given a set of data that changes very infrequently, create a set of web pages that display the details of each record in the dataset. Simple right?</p>
<p>Now, there's two ways I could have built out this functionality:</p>
<ol>
<li>Dynamic pages that serve the content using templates served by Django</li>
<li>Static pages built once and served by Nginx</li>
</ol>
<details>
  <summary>Additional Context</summary>
  <p>I wanted this functionality to be separate from our main project, so the first solution would involve setting up a new Django app and database as well as building a system to import the data from JSON files.</p>
</details>

<p>I elected to do the second solution because of it's simplicity and because the data changes so rarely. When all was said and done, I had three scripts &mdash; two bash scripts, and one Python script &mdash; and a set of <a href="https://jinja.palletsprojects.com/en/3.0.x/">Jinja templates</a>. Running the main build script would download the dataset if it didn't already exist, parse the data, and build the pages. The whole process takes about 10 seconds to download and generate over 1200 pages. After it was done, I even set up a Docker container to build and serve the pages with Nginx. In total the project is 295 lines of code (1k if you include the HTML templates), and basically never needs to be updated again; if the data does ever change, we could simply re-run the script or rebuild the contaner.</p>
<h2>Scripting is a Much-Needed Break</h2>
<p>This is one thing I love about scripting as opposed to application development. In app development, you construct a codebase and then you need to live with it long term, adding new functionality and deprecating old functionality. Scripts, on the other hand, are low risk, high reward: you write them once to solve a specific problem and then rarely touch them again. I use a lot of simple scripts on a daily basis, some of which I wrote nearly a decade ago and haven't touched since.</p>
<p>Scripts are programming candy whereas app development is the real meat and potatoes. In a script you can take shortcuts, be a bit messy, and forgo worrying about the complexities of large software. Once the script works, there's not much else to do: just ship it.</p>
<p>Whenever I get the chance to take a break from developing large apps and just do some quick scripting, I leap at the opportunity.</p>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/the-road-to-glass--stone</guid><pubDate>Tue, 07 Dec 2021 21:11:00 -0800</pubDate><category>music</category><title>The Road to Glass &amp; Stone</title><link>http://brianschrader.com/archive/the-road-to-glass--stone/</link><description><![CDATA[<p>I don't think I've ever talked about this here before, but I play in a band. And that band, <a href="https://thefourthsection.com">The Fourth Section</a>, just released its first EP, 'Glass &amp; Stone'; and it's available now.</p>
<p>It's been almost two years since we formed, just before lockdown in March of 2020, so technically this is our pandemic EP. I'm super happy that we've finally gotten a release out there and available for everyone.</p>
<p><a href="https://thefourthsection.com"><img
    src="/images/blog/glass_and_stone.png"
    alt="Glass & Stone"
    class="image-center"
/></a></p>
<p>I've released music on Bandcamp before, but this is the first time that I'd ever gone through the process of getting music onto the big platforms. It's both a time-consuming process, and still incredible that a bunch of indie artists, with no label backing, can simply fill out some paperwork and then distribute music world wide. We often lose sight of it, but the internet is really magical sometimes.</p>
<p>The EP was recorded, mixed, and mastered at <a href="https://www.cachoestudio.com">Cacho Studio</a> in Tijuana, and they did a great job. My thanks to everyone at the studio for their hard work.</p>
<h2>A Virtual and Untrodden Road</h2>
<p>Since we formed right before the pandemic lockdowns hit the U.S., we couldn't practice at a studio in-person at first. For nearly a year we rehearsed virtually or acoustically&mdash;as well as masked and social distanced&mdash;at a local park. The three of us used <a href="https://jamkazam.com">Jamkazam</a> for our virtual practices and it works well enough for that purpose, but it's no substitute for actual, plugged-in rehearsal at a practice studio.</p>
<p>We're back to in-person rehearsal now, and it feels great. Lots of new stuff is in the works and hopefully there will be more to show in the coming months.</p>
<p>Having gone through the process of writing, composing, and distributing music once, we now have a pretty good idea of what it takes to make an EP (or an album) happen, and we're really stoked to do this whole process again (and soon). As with most things, the first try is usually the hardest. There's always the initial, one-time setup that has to be done, and there's just so much you don't know the first time. That is all past now. We know what it takes, and the setup is done. Releasing more stuff is a lot easier now.</p>
<p>And that is what we plan to do.</p>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/nine9s-gets-an-upgrade-</guid><pubDate>Mon, 06 Dec 2021 18:28:00 -0800</pubDate><category>software development</category><category>apps</category><title>Nine9s gets an upgrade ⬆️</title><link>http://brianschrader.com/archive/nine9s-gets-an-upgrade-/</link><description><![CDATA[<p>I'm super excited to announce a new suite of features for <a href="https://nine9s.cloud/">Nine9s: my uptime monitoring service</a>. Now, I don't want to exaggerate too much, but I honestly thing that as of today, Nine9s is one of the most powerful uptime monitoring services out there; if you haven't already, you should try it out.</p>
<h2>Monitor Anything</h2>
<p>Uptime monitoring, in most cases is a pretty simple task: if you have a web service, then Nine9s makes a request to it, and verifies that it returns a successful HTTP status code. Most services stop there, or they expand on the idea and allow for checks to be more complex (e.g. verifying that elements exist on the page or that users can log in).</p>
<p>The problem with this approach is that lots of services exist on the web, but do things other than simply serve web content. You could have a service that sends emails or that runs batch jobs. These services can't be monitored using simple HTTP requests. Even still, sometimes you don't want your services to be publicly accessable, in which case uptime monitoring services can't reach out to them at all.</p>
<p>That's where Nine9s steps in to fill the gap! As of today, servers can now periodically ping Nine9s to report that they're still up and running. Nine9s will watch out for these reports and let the user know if the service doesn't report in within a given amount of time.</p>
<p>Better still, Nine9s can evaluate the data sent by the server and compare it against some criteria. Imagine you have your server report its current disk usage to Nine9s. If the disk ever fills up past say 95%, then Nine9s will alert you immediately.</p>
<p><img alt="Example Criteria on Nine9s" src="/images/blog/nine9s-criteria.png" /></p>
<p>When you set up a new Measure&mdash;which is what Nine9s calls these new features&mdash;Nine9s will generate a couple of sample cron jobs you can quickly add to your servers to get started! There's even a <a href="https://nine9s.cloud/kb/measurement-snippets">quickstart guide</a> to writing additional jobs!</p>
<p><img alt="Sample Cron Scripts" src="https://nine9s.cloud/static/kb/cron.png" /></p>
<p>Once you understand the power of these new Measures, you'll be able to monitor nearly anything your servers are doing and make sure they're performing as expected.</p>
<h2>Nine9s is macOS Native!</h2>
<p>This release also marks the launch of the <a href="https://nine9s.cloud/#app">official Nine9s app for macOS!</a> It's my first ever public macOS app and I'm super excited to get it out there into the world. The app shares all of its code with the iOS and iPad versions  and is built on Catalyst, so it should feel <em>mostly</em> native.</p>
<p>The app has all of the features you'd imagine: the ability to monitor your Endpoints and Measures, support for push notifications, and more.</p>
<p>The app is also my first ever native app released outside of the App Store!</p>
<p>I've been working on this update for a few months now and I'm stoked to get it into the hands of all you out there! If you do end up downloading the app, let me know especially if you have feature recommendations! I'd also love to know how you all are using Measures in the wild.</p>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/the-internets-original-sin</guid><pubDate>Tue, 26 Oct 2021 18:07:00 -0800</pubDate><category>software</category><category>internet</category><category>history</category><title>The Internet's Original Sin</title><link>http://brianschrader.com/archive/the-internets-original-sin/</link><description><![CDATA[<p>A piece I wrote was <a href="https://www.sandiegouniontribune.com/opinion/commentary/story/2021-10-25/internet-website-domain-privacy-big-data">published in the San Diego Union Tribune</a> today. The piece is a short dive into a possible alternate history of the web and the internet as it could have been.</p>
<p>In a bucolic way, the piece compares the development of the internet and the inevitable consequences of a few seemingly insignificant policies adopted by ISPs in the early days. The piece draws a through-line from those policies straight to the emergence of the centralized platforms we know today.</p>
<p>In short: ISPs killed the internet and made platforms like Facebook and Amazon inevitable.</p>
<p>The piece compares America's idealized conception of the westward expansion on the frontier to the development of the internet in the last few decades.</p>
<blockquote>
<p>There are no homesteads on the internet frontier. This is true on both the large and the small scales. Whether you start a website yourself, or use Facebook, you’re still effectively renting space from someone else. On the internet, <strong>we are all simply users of other people’s computers, tenants renting homes we can never own.</strong></p>
</blockquote>
<p><img
    src="/images/blog/ut-paper-tech-article.jpg"
    alt=""
    class="image-center"
/></p>
<div class="text-center"><small><caption>
The title for the print version is much more attention grabbing.
</caption></small></div>

<p>However, the development of the internet was fundamentally different from westward expansion in two key ways. First off, there was no one already living on the internet frontier&mdash;which isn't relevant to the piece directly, but it's an important distinction. The expansion onto the internet frontier was bloodless.</p>
<p>But secondly, ISPs made the internet a renter's frontier. Their decisions to not assign static IP addresses to homes or user accounts, and their decisions to provide non-symmetrical connections and block ports 80 and 443 made the internet fundamentally different than it was intended. On the internet today, truly owning your online presence is only possible for a very small subset of individuals and companies; the landlords of the internet.</p>
<blockquote>
<p>No one would accept a world in which you could never own the car you drive, or the house you live in. Some people prefer to rent or lease, but cars and houses can still actually be bought. On the internet there is no path to ownership. Each of these stumbling blocks ensures that ordinary people can never truly own a piece of the internet frontier.</p>
<p>If computers are the land of the internet, then our personal information is the crop. Instead of being given our own small farm to start our online lives anew, we’re forced to be digital sharecroppers. In exchange for a portion of our privacy, we are allowed the chance to cultivate lands we do not own. On the internet, we are not citizens. We’re just users. Is this freedom?</p>
</blockquote>
<p><a href="1">Check out the full piece for yourself &#8594;</a></p>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/last-year-i-started-reading-a-physical-newspaper</guid><pubDate>Sat, 09 Oct 2021 01:00:00 -0800</pubDate><category>newspapers</category><category>writing</category><title>Last Year I Started Reading a Physical Newspaper</title><link>http://brianschrader.com/archive/last-year-i-started-reading-a-physical-newspaper/</link><description><![CDATA[<p>I subscribe to a lot of local and national news outlets, and as most people do these days, I read the news on a screen, because well, I'm a 30 year old millennial and it's <span id="now-dt">2021</span> and not 1980. But last September, out of a mix of both idle curiosity, <a href="https://www.dictionaryofobscuresorrows.com/post/105778238455/anemoia-n-nostalgia-for-a-time-youve-never">anemoia</a>, and a strong urge to put an end to pandemic-induced doom-scrolling, I signed up to receive a printed copy of the Sunday paper.</p>
<p>I've been happily reading a Sunday paper ever since.</p>
<p><del>Every Sunday</del> Most Sundays, a paper appears before I wake up, I stumble, half asleep, out to get it, I make coffee, and then I read the news. It's a quaint experience, and it's a habit I've really come to enjoy and look forward to. The whole process takes anywhere from 30-90 minutes, then it's done, and I go about my day. On Sundays I do my absolute best not to read the news on my phone, as I do every other day. Once I've read the paper, I'm done reading the news.</p>
<p><img
    alt="Newspapers on a surface"
    src="/images/blog/sd-ut.jpg"
    class="image-center"
/><div class="text-center"><small>A stack of papers I had lying around. Photo credit: me.</small></div></p>
<p>Part of the reason I originally signed up for a physical paper was to force myself to read it. As I said, I subscribe to a lot of new sites, and I find a lot of articles that I want to read, but I almost never actually read them all.</p>
<p>With so many stories to read, I found myself naturally gravitating towards the most exciting or most outrageous news. National news is almost always much more polarizing, engaging, and exciting than state and local news, but the paradox is that state and local news is often far more important and more likely to directly influence you in your day-to-day life.</p>
<p>What's exciting, urgent, and engaging is rarely what's important. I noted this in a previous post about my policy blog, Democracy &amp; Progress.</p>
<blockquote>
<p>A few years back, while listening to the Ezra Klein Show, Ezra lamented that we as a society didn't spend more time focusing on local and state politics—where our time and energy is often better spent. Collectively, we don't focus on state and local politics, and yet it's only there where a lot of policy solutions can be done. That conversation stuck with me...<br />
&ndash; <a href="/archive/california-democracy-and-progress/">California, Democracy, and Progress</a></p>
</blockquote>
<p>I found that having a physical paper show up at my door made me more likely to read it. Simply throwing it away felt wasteful.<sup>1</sup> That slight guilt motivates me to keep to my own goals.</p>
<p>In the last year, my knowledge of local &amp; state politics has grown immensely, and I've really enjoyed the process of learning more about my state and city. I also know a lot more about both San Diego and California and what progress is and isn't being made.</p>
<p>Lots of people both inside and outside of the media lambast the news industry for being overly negative, and while it certainly is, national outlets are catering to a much more diverse and varied reader-base. On top of that, almost any political news story in a country as large and sufficiently polarized as the U.S. is bound to upset around 50% of readers. A political news story out of Michigan, Utah, or Texas (to pick three states at random) will likely upset Californians and vice versa. State and local news is more likely to comport with your views&mdash;assuming you somewhat agree with your community at-large.</p>
<p>I often discuss the news with friends and I usually get asked some version of the question, "How can you read the news so much? Isn't it just depressing?"</p>
<p>It can be, sure. But California has done quite a few things lately that I really like, and there's a lot of potential here for us to tackle big problems. If you're constantly focused on national news, it can feel like nothing ever gets done (because nationally nothing ever gets done), but locally that's just not true. Huge things are happening in California, and it feels good to know what they are.</p>
<p>I've also learned a lot about things I didn't think I would be interested in. The internet has trained us to seek filter bubbles and echo chambers, and to retreat into our known interests. A newspaper, because of its physical constraints, cannot be all things to all people. There are often pages of articles about topics I wouldn't say I care about, and yet I find myself reading them. On Sundays I find myself learning about up-and-coming bands, concerts, local events, and investing tips whether I wanted to or not, and it's been a really positive experience; not universally positive, but still very positive. Echo chambers aren't great, and physical papers are a good escape from them.</p>
<p>I'm not sure I had much of a high-minded conclusion to this retrospective, but I can say that I've enjoyed the anachronistic ritual of reading a physical paper, and I expect to continue doing so for a while.</p>
<div class="footnote">
<sup>1.</sup> They all get recycled. Don't at me.
</div>

<script>
document.getElementById('now-dt').innerHTML = (new Date()).getFullYear();
</script>]]></description></item><item><guid isPermaLink="true">http://brianschrader.com/archive/automated-podcasts-with-automator-amp-overcast</guid><pubDate>Tue, 24 Aug 2021 20:59:00 -0800</pubDate><category>blogging</category><category>programming</category><title>Automated Podcasts with Automator &amp;amp; Overcast</title><link>http://brianschrader.com/archive/automated-podcasts-with-automator-amp-overcast/</link><description><![CDATA[<p>I've <a href="/archive/siri-is-a-blogging-tool/">mentioned before</a> that I use Siri as an editing tool. I write a piece, lightly edit it, and then have Siri read it back to me. This helps me catch unintended grammatical errors and clumsy sentences. Building on that principle, <a href="https://pine.blog">Pine.blog</a> and <a href="https://hewellapp.com">Hewell</a> both ship with a feature that use iOS's <code>AVSpeechSynthesizer</code> API to read articles or location information aloud.</p>
<p>That said, I often find articles that I want to read, but after a long day staring at a computer screen, I don't want to actually <em>read</em> them. Lots of sites these days provide spoken audio for their articles&mdash;which is great&mdash;but the vast majority don't.</p>
<p>That's where Automator comes in.</p>
<p><img
    alt="Save Spoken Text to File"
    src="/images/blog/automator-spoken-text.png"
    style="width:500px; height:381.5px;"
    class="image-center"
/></p>
<p>This Automator service simply runs a bash script that takes the contents of the selected text as input, feeds it to the built-in macOS <code>say</code> command, and outputs it to a file on the Desktop named using the contents in my clipboard.</p>
<details>
  <summary>Check out the full script</summary>
<pre><code>cd ~/Desktop;
# A hack to get stdin into say through Automator. For some
# reason simply saying -f didn't work for me.
while read line; do echo "$line" done < "${1:-/dev/stdin}" |
    say -o .spoken_text -f -

TITLE="$(pbpaste -Prefer txt)"
if [ -z "$TITLE" ]; then
    TITLE="Spoken Text"
fi
# Sanitize the article title. Writers love colons which macOS hates
TITLE="$(echo "$TITLE" | sed -e 's/[^A-Za-z0-9._-]/_/g')"

# Conver the audio and be quiet about it
/usr/local/bin/ffmpeg -i .spoken_text.aiff -loglevel -8 -y "$TITLE.aac"
rm .spoken_text.aiff</code></pre>
</details>

<p>The script also uses FFmpeg to convert the audio to an AAC file so that I can then upload it to <a href="https://overcast.fm">Overcast</a>, my preferred podcast player.</p>
<p>By default, macOS will include Automater services in the right-click menu, but I've also bound the script to <code>Cmd+Ctl+Shift+S</code> (which is similar to my existing <code>Cmd+Ctl+S</code> shortcut for reading the selected text aloud).</p>
<p><img
    alt="The macOS Services Menu"
    src="/images/blog/services-menu-speak.png"
    style="width:321px; height:230px"
    class="image-center"
/></p>
<p>Now, I can discover new articles to read, perform a quick set of keystrokes, upload the audio to Overcast, and then go for a walk while I catch up on the day's interesting news!<sup>1</sup></p>
<p>I've provided the Automator service as a zip archive below if anyone wants to play with it.</p>
<p><a href="/dropzone/Save Spoken Text to File.zip">⬇️ Save Spoken Text to File.workflow</a></p>
<hr />
<div class="footnote">
    <p><sup>1.</sup>&nbsp;There are a few quirks to this workflow still. Websites are filled with non-article content, so to avoid selecting those, I typically following the following steps:
    </p>
    <ol>
        <li>Turn on reader mode (<code>Cmd+Shift+R</code>)</li>
        <li>Copy the title of the article to the clipboard (<code>Cmd+C</code>)</li>
        <li>Select the article text (<code>Cmd+A</code>)</li>
        <li>Run my Automator service (<code>Cmd+Ctl+Shift+S</code>)</li>
        <li>Upload the new AAC file to Overcast</li>
    </ol>
    <p>I admit, it's a little cumbersome, but it does work really well.</p>
</div>]]></description></item></channel></rss>